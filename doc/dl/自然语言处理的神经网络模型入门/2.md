## 二、神经网络架构

神经网络是强大的学习模型。 我们将讨论两种可混合和匹配的神经网络架构 - 前馈网络和循环/递归网络。 前馈网络包括具有完全连接层的网络，例如多层感知器，以及具有卷积层和池化层的网络。 所有网络都充当分类器，但每个网络都有不同的优势。

全连接的前馈神经网络（第 4 节）是非线性学习器，在大多数情况下，可以用作任何使用线性学习器的普适性替代。这包括二元和多类分类问题，以及更复杂的结构化预测问题（第 8 节）。网络的非线性，以及容易整合预训练的词嵌入的能力，通常产生更好的分类准确性。一系列的工作（Chen & Manning，2014；Weiss，Alberti，Collins，& Petrov，2015；Pei，Ge，& Chang，2015；Durrett & Klein，2015）仅仅通过使用全连接的前馈网络的解析器替换线性模型，获得了改进的句法分析结果。前馈网络作为分类替代方案的直接应用（通常与预训练的词向量相结合）也为 CCG 超极标注（Lewis & Steedman，2014），对话状态跟踪（Henderson，Thomson， ，2013年），统计机器翻译的预排序（de Gispert，Iglesias，& Byrne，2015）和语言建模（Bengio，Ducharme，Vincent，& Janvin，2003；Vaswani，Zhao，Fossum，& Chiang，2013）提供了好处。 Iyyer 等（2015）证明多层前馈网络可以在情感分类和真实问题回答方面提供有竞争力的结果。

具有卷积层和池化层的网络（第 9 节）对于这样分类任务很有用，其中我们希望找到类成员的强大局部线索，但是这些线索可能出现在输入的不同位置。例如，在文档分类任务中，单个关键短语（或 ngram）可以有助于确定文档主题（Johnson & Zhang，2015）。我们想知道某些字词序列是否该主题的良好指标，并且不一定关心它们在文档中的出现位置。卷积和池化层允许模型学习如何找到这样的局部指标，而不管它们的位置。卷积和池化架构在许多任务中表现出了令人满意的结果，包括文档分类（Johnson & Zhang，2015），短文本分类（Wang，Xu，Xu，Liu，Zhang，Wang 和 Hao，2015a），情感分类（Kalchbrenner, Grefenstette, & Blunsom, 2014; Kim, 2014），实体之间的关系类型分类（Zeng, Liu, Lai, Zhou, & Zhao, 2014; dos Santos, Xiang, & Zhou, 2015），事件检测（Chen, Xu, Liu, Zeng, & Zhao, 2015; Nguyen & Grishman, 2015），短语识别（Yin & Sch¨utze, 2015），语义角色标注（Collobert，Weston，Bottou，Karlen，Kavukcuoglu，& Kuksa，2011），问答（Dong, Wei, Zhou, & Xu, 2015），根据评论预测电影的票房收入（Bitvai & Cohn, 2015）建模文本的兴趣度（Gao，Pantel，Gamon，He 和 Deng，2014），字符序列和词性标签之间的关系（Santos & Zadrozny，2014）。

在自然语言中，我们经常使用任意大小的结构化数据，如序列和树。我们希望能够捕捉这些结构中的规律性，或者模拟这些结构之间的相似性。在很多情况下，这意味着将结构编码为一个固定宽度的向量，然后我们可以将其传递给另一个统计学习器，进行进一步处理。虽然卷积和池化架构允许我们将任意大型项目编码为固定大小的向量，捕获其最显着的特征，但它们通过牺牲大部分结构信息来实现。另一方面，循环（第 10 节）和递归（第 12 节）架构允许我们在保留大量结构信息的同时处理序列和树。循环网络（Elman，1990）用于对序列进行建模，而递归网络（Goller＆Küuler，1996）是可以处理树的循环网络的推广。我们还将讨论你能够模拟堆栈的循环网络的扩展（Dyer，Ballesteros，Ling，Matthews，& Smith，2015；Watanabe & Sumita，2015）。

循环模型已经表明，对语言建模产生了非常强的结果，包括（Mikolov，Karafi'at，Burget，Cernocky，& Khudanpur，2010; Mikolov，Kombrink，Luk'as Burget，Cernocky，& Khudanpur，2011; Mikolov，2012 ; Duh，Neubig，Sudoh 和 Tsukada，2013; Adel，Vu 和 Schultz，2013; Auli，Galley，Quirk 和 Zweig，2013; Auli & Gao，2014）; 以及序列标注（Sundermeyer，Alkhouli，Wuebker，2014）; 机器翻译（Sundermeyer, Alkhouli, Wuebker, & Ney, 2014; Tamura, Watanabe, & Sumita, 2014;
Sutskever, Vinyals, & Le, 2014; Cho, van Merrienboer, Gulcehre, Bahdanau, Bougares,
Schwenk, & Bengio, 2014b），依赖解析（Dyer 等，2015; Watanabe & Sumita，2015），情感分析（Wang，Liu，SUN，Wang 和 Wang，2015b），嘈杂文本规范化（Chrupala，2014），对话状态跟踪（Mrksi'c，OS'eaghdha，Thomson， （Sordoni，Galley，Auli，Brockett，Ji，Mitchell，Nie，Gao，＆Dolan，2015），以及建模字符序列和词性标签的关系（Ling 等，2015b）。

递归模型被证明可以为选区（Socher，Bauer，Manning，& Andrew Y.，2013）和依赖性（Le & Zuidema，2014; Zhu 等，2013），语音关系分类（Hashimoto，Miwa，Tsuruoka，& Chikayama，2013; Liu，Wei，Li，Ji，2015a）分析重排，语篇分析（Li，Li & Hovy，2014） 基于分析树的政治意识形态检测（Iyyer，Enns，Boyd-Graber，& Resnik，2014b），情感分类（Socher，Perelygin，Wu，Chuang，Manning，Ng，& Potts，2013; Hermann & Blunsom，2013），目标依赖情感分类（Dong，Wei，Tan，Tang，Zhou，& Xu，2014）和问题回答（Iyyer，Boyd-Graber，Claudino，Socher 和 Daum'e III，2014a）提供最先进的或近乎最先进的结果。

