## 四、前馈神经网络

大脑启发的隐喻。顾名思义，神经网络受大脑计算机制的启发，该机制由称为神经元的计算单元组成。 在隐喻中，神经元是一个具有标量输入和输出的计算单位。 每个输入都有相关的权重。 神经元将每个输入乘以它的权重，然后求和它们 [4]，对结果应用一个非线性函数，并将其传递给它的输出。 神经元相互连接，形成一个网络：神经元的输出可以馈入一个或多个神经元的输入。 据证明，这种网络是非常有能力的计算设备。 如果权重设置正确，那么具有足够神经元和非线性激活函数的神经网络，可以逼近非常广泛的数学函数（我们后面会更精确）。

> [4] 求和是最常见的操作，其它操作，比如最大值，也是可能的。

![](img/2.jpg)

图 2：带有两个隐层的前馈神经网络

一个典型的前馈神经网络可能如图 2 所示。每个圆圈都是一个神经元，其中输入箭头是神经元的输入，输出箭头是神经元的输出。 每个箭头都带有权重，反映其重要性（未显示）。 神经元分层排列，反映信息流。 底层没有传入的箭头，并且是网络的输入。 顶层没有输出箭头，并且是网络的输出。 其他层被认为是“隐藏的”。中层神经元内的 S 形表示一个非线性函数（通常是 $\sigma(x) = 1 / (1 + e^{-x})$，但有更好的函数），将神经元的值传递给 输出之前，它应用于神经元的值。图中，每个神经元连接到下一层的所有神经元（这称为全连接层或仿射层）。

虽然大脑隐喻性感而有趣，但从数学上操作，它也是分散注意力和繁琐的。 因此，我们转而使用更简洁的数学符号。 网络中每行神经元的值可以当做是一个向量。 在图 2 中，输入层是一个 4 维向量（`x`），而它上面的层是一个 6 维向量（$h^1$）。 全连接层可以当做从 4 维到 6 维的线性变换。全连接层实现向量矩阵乘法，$h = xW$，其中 $W_{ij}$ 是从输入行中的第`i`个神经元到输出行中的第`j`个神经元的连接权重 [5]。然后，`h`的值被非线性函数`g`转换，它在传递到下一个输入之前应用于每个值。 从输入到输出的整个计算可写为：$g(xW^1)W^2$，其中 $W^1$ 是第一层的权重，$W^2$ 是第二层的权重。

> [5] 为了弄清为什么是这样，将`h`中第`j`个神经元的第`i`个输入表示为 $w_{ij}$。$h_j$ 的值就为 $h_j = \sum^4_{i=1} x_i \cdot w_{ij}$。

在数学符号中。从这一点开始，我们将放弃大脑隐喻，并专门用向量矩阵运算来描述网络。 最简单的神经网络是感知器，它是其输入的线性函数：

![](img/tex-3.jpg)

`W`是权重矩阵，`b`是偏差项 [6]。为了超越线性函数，我们引入了一个非线性隐层（图 2 中的网络有两个这样的层），产生了 1 层的多层感知机（MLP 1）。 单层前馈神经网络的形式如下：

> [6] 图2中的网络不包含偏差项。 可以通过向其添加一个没有任何传入连接的额外神经元，来将偏置项添加到层，其值始终为 1。

![](img/tex-4.jpg)

这里 $W^1$ 和 $b^1$ 是用于输入的第一个线性变换的矩阵和偏置项，`g`是逐元素应用的非线性函数（也称激活函数），并且 $W^2$ 和 $b^2$ 是第二个线性变换的矩阵和偏差项。

将其分解，$xW^1 + b^1$ 是输入`x`从 $d_{in}$ 尺寸到 $d^1$ 尺寸的线性变换。 然后将`g`应用于每个 $d^1$ 维度，然后将矩阵 $W^2$ 与偏差向量 $b^2$ 一起用于将结果转换为 $d^2$ 维输出向量。 在网络表示复杂功能的能力中，非线性激活函数`g`起着至关重要的作用。 如果没有`g`中的非线性，神经网络只能表示输入的线性变换 [7]。

> [7] 为了弄清楚为什么，考虑一系列线性变换还是线性变换。

我们可以增加额外的线性变换和非线性，产生 2 层 MLP（图 2 中的网络是这种形式的）。

![](img/tex-5.jpg)

使用中间变量编写像这样的更深层的网络也许更清楚：

![](img/tex-6.jpg)

由每个线性变换产生的向量被称为层。 最外面的线性变换产生输出层，而其他线性变换产生隐层。 每个隐层后面都有一个非线性激活。 在某些情况下，比如在我们的例子的最后一层，偏置向量被强制为 0（丢弃了）。

线性转换产生的层通常被称为全连接或仿射。其他类型的架构也存在。 特别是，图像识别问题受益于卷积层和池化层，这些层也用于语言处理，将在第 9 节讨论。具有多于一个隐层的网络被认为是深度网络，因此称为深度学习。

在描述神经网络时，应该指定层的尺寸和输入。一个图层将期望 $d_{in}$ 维向量作为其输入，并将其转换为 $d_{out}$ 维向量。该层的维度被视为其输出的维度。对于输入维数为 $d_{in}$，输出维数为 $d_{out}$ 的全连接层 $l(x)= xW + b$，`x`的维数为 $1 \times d_{in}$，`W`为 $d_{in} \times d_{out}$，`b`的维数为 $1 \times d_{out}$。

网络的输出是一个 $d_{out}$ 维的向量。在 $d_{out} = 1$ 的情况下，网络的输出是标量。这种网络可以通过考虑输出值来进行回归（或评分），或者通过查询输出的符号来进行二元分类。 $d_{out} = k > 1$ 的网络可用于`k`元分类，方法是将每个维与一个类相关联，然后查找具有最大值的维。与之类似，如果输出向量条目是正数并且总和为 1，则输出可以被解释为类别上的分布（这种输出归一化通常通过在输出层上应用 softmax 变换来实现，参见 4.3 节）。

定义线性变换的矩阵和偏差项是网络的参数。通常将所有参数的集合称为`θ`。参数与输入一起决定网络的输出。训练算法负责设置它们的值，使网络的预测是正确的。训练在第 6 节中讨论。

## 4.1 表现力

在表现力方面，（Hornik，Stinchcombe，& White，1989; Cybenko，1989）表明 MLP1 是一个通用逼近器 - 它可以以任意期望的非零误差量，近似一系列函数 [8] 包括 $R ^n$ 的闭合和有界子集上的所有连续函数，以及从任何有限维离散空间到另一个有限维离散空间的任何函数映射。这可能意味着，没有理由去做超越 MLP1 的更复杂的体系结构。然而，理论结果并没有说明隐藏层应该有多大，也没有说明神经网络的学习能力（它只是说表示是存在的，但没有说明，基于训练数据和特定的学习算法，设置参数有多容易或困难）。它也不能保证，训练算法会找到生成我们训练数据的正确函数。因为在实践中我们使用反向传播算法和随机梯度下降变体的组合，并且使用相对适中大小的隐藏层（高达数千），在相对少量数据上训练神经网络，因此尝试比 MLP1 更复杂的体系结构是有帮助的。然而，在许多情况下，MLP1 确实提供了非常有力的结果。前馈神经网络表示能力的进一步讨论，请参阅（Bengio 等，2015，第 6.5 节）。

## 4.2 常见的非线性函数

非线性`g`可以采取多种形式。 目前还没有很好的理论来说明，应用哪种非线性条件，以及为给定任务选择正确的非线性函数大部分是经验问题。 现在我将回顾文献中常见的非线性：sigmoid，tanh，hard tanh和整流线性单元（ReLU）。 一些 NLP 研究人员还尝试了其他形式的非线性，例如 cube 和 tanh-cube。

Sigmoid：sigmoid 激活函数 $\sigma(x) = 1 / (1 + e^{-x})$ 是 S 形函数，将`x`的每个值转换到范围`[0,1]`中。

双曲正切（tanh）：双曲正切 $tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1}$ 激活函数是 S 形函数，将`x`的每个值转换到范围`[-1,1]`中。

Hard tanh：Hard tanh 激活函数是 tanh 函数的近似，它计算更快，并且衍生为：

![](img/tex-7.jpg)

整流器（ReLU）：整流器激活函数（Glorot，Bordes 和 Bengio，2011）也称为整流线性单元，它是一种非常简单的激活函数，易于使用，并且多次表明可以产生出色的结果 [9]。ReLU 单元在 0 处截断每个`x <0`的值。尽管简单，但它在很多任务中表现良好，特别是与 dropout 正则化技术结合使用时（见 6.4 节）。

![](img/tex-8.jpg)

作为经验之谈。ReLU 单元比 tanh 好，tanh 比 sigmoid 好 [10]。

## 4.3 输出转换

许多情况下，输出层向量也会转换。常见的转换是 softmax：

![](img/tex-9.jpg)

结果是和为 1 的非负实数的向量，使其成为`k`个可能结果上的离散概率分布。 

当我们有兴趣对可能的输出类别进行概率分布建模时，使用 sof tmax 输出变换。 为了有效，它应该与例如交叉熵的概率性训练目标结合使用（见下面的第 4.5 节）。 

当 softmax 变换应用于没有隐藏层的网络输出时，结果是众所周知的多项逻辑回归模型，也称为最大熵分类器。

## 4.4 内嵌层

到目前为止，讨论忽略了`x`的来源，将其视为任意向量。 在 NLP 应用中，`x`通常由各种嵌入向量组成。 我们可以明确`x`的来源，并将其包含在网络的定义中。 我们引入了`c(·)`，这是一个从核心特征到输入向量的函数。 `c`通常提取与每个特征相关的嵌入向量，并将它们连接起来：

![](img/tex-10.jpg)

`c`的另一个常见选择是对嵌入向量求和（这假设嵌入向量共享相同）。

![](img/tex-11.jpg)

`c`的形式是网络设计的重要组成部分。 在许多论文中，通常将`c`指为网络的一部分，并且同样将嵌入 $v(f_i)$ 的单词视为由“嵌入层”或“查找层”产生的结果。 考虑一下单词`|V|`的词汇表，每个词嵌入为一个`d`维向量。 然后可以将向量集合视为`|V| ×d`嵌入矩阵`E`，其中每行对应于嵌入特征。 假设 $f_i$ 是一个`|V|`维向量，它除了一个下标外全部为零，下标对应于第`i`个特征的值，其值为 1（这被称为单热向量）。 然后乘法 $f_iE$ 将“选择”`E`的对应行。因此，$v(f_i)$ 可以用`E`和 $f_i$ 来定义：

![](img/tex-12.jpg)

与之相似：

![](img/tex-13.jpg)

网络的输入被认为是单热向量的集合。 虽然这在数学上是优雅的且明确定义的，但高效的实现通常涉及基于散列的数据结构，将特征映射到其对应的嵌入向量，而不经历单热表示。 

在本教程中，我们将`c`与网络架构分开：网络的输入始终是密集的实值输入向量，在输入通过网络之前应用`c`，类似于熟悉的线性模型术语中的“特征函数”。 但是，在训练网络时，输入向量`x`确实记得它是如何构建的，并且可以将误差梯度传播回其成分，嵌入向量。

符号注释。当描述网络层，它将连接的向量`x`，`y`和`z`作为输入的时，有些作者使用显式连接（`[x; y; z] W + b`），而另一些使用仿射变换（`xU + yV + zW + b`）。 如果仿射变换中的权重矩阵`U`，`V`，`W`彼此不同，则这两个符号是等价的。

稀疏特性和密集特性的注释。考虑一个网络，该网络的输入向量使用“传统”稀疏表示，并且没有嵌入层。假设所有可用特征的集合是`V`，并且我们有“特征 $f_1, ..., f_k, f_i \in V$，网络的输入是：

![](img/tex-14.jpg)

所以第一层（忽略非线性激活）是：

![](img/tex-15.jpg)

该图层选择对应于`x`中输入特征的`W`行，并对它们进行求和，然后添加一个偏差项。 这非常类似于在特征上产生 CBOW 表示的嵌入层，其中矩阵`W`充当嵌入矩阵。 主要区别在于引入了偏置向量`b`，并且嵌入层通常不经历非线性激活，而是直接传递到第一层。 另一个区别是，这种情况强迫每个特征接收一个单独的向量（`W`中的行），而嵌入层提供更多的灵活性，例如允许特征“next word is dog”和“previous word is dog”共享相同向量。 但是，这些差异很小且微妙。 对于多层前馈网络来说，密集输入和稀疏输入之间的差异小于它看起来的样子。

## 4.5 损失函数

当训练一个神经网络时（更多请见下面第 6 节中的训练），就像训练一个线性分类器时一样，定义了一个损失函数 $L(\hat y,y)$，表示当真实输出是`y`时预测 $\hat y$ 的损失。 然后训练目标是尽量减少不同训练样本中的损失。 给定真实期望输出`y`，损失 $L(\hat y,y)$ 为网络输出 $ hat y$ 分配数字分数（标量）[11]。 损失总是正的，只有在网络输出正确的情况下才应该为零。 

然后设置网络的参数（矩阵 $W^i$，偏差 $b^i$ 和通常的嵌入`E`），以便最小化训练样例上的损失`L`（通常，它是被最小化的，不同训练样例上的损失总和）。

损失可以是将两个向量映射到标量的任意函数。 为了优化的实际目的，我们将自己限制在可以轻松计算梯度（或次梯度）的函数中。 在大多数情况下，依靠常见的损失函数而不是定义自己的函数是可靠的。 神经网络损失函数的详细讨论，参见（LeCun，Chopra，Hadsell，Ranzato，& Huang，2006; LeCun & Huang，2005; Bengio 等，2015）。 我们现在讨论一些神经网络中常用的 NLP 损失函数。

Hinge（二元）。对于二元分类问题，网络的输出是单个标量 $\hat y$，并且预期输出`y`在`{+1，-1}`中。 分类规则是`sign(y)`，如果 $\hat y \cdot y > 0$，则分类被认为是正确的，这意味着 $\hat y$ 和`y`共享相同的符号。 Hinge 损失也称为边际损失或 SVM 损失，定义为：

![](img/tex-16.jpg)

当 $\hat y$ 和`y`共享相同的符号和 $|y| \le 1$ 时，损失为 0，否则，损失是线性的。 换句话说，二元 Hinge 损失试图实现正确的分类，边距至少为 1。

Hinge（多类）损失由 Crammer 和 Singer（2002）扩展到多分类设置。 令 $\hat y = y_1, ..., \hat y_n$ 是网络的输出向量，`y`是正确输出类的一个热点向量。 分类规则被定义为选择分数最高的分类：

![](img/tex-17.jpg)

用 $t = arg max_i \hat y_i$ 表示正确的类别，并且用 $k = arg max_{i \ne t} y_i$ 表示得分最高的类别，满足 $k \ne t$。 多类 Hinge 损失定义为：

![](img/tex-18.jpg)


多类 Hinge 损失试图在所有其他类别之上对正确类别进行评分，边距至少为 1。二元和多类别 Hinge 损失旨在与线性输出层一起使用。 无论何时我们需要硬判别规则，Hinge 损失都很有用，并且不会尝试对类成员进行概率建模。 

对数损失。对数损失是 Hinge 损失的常见变化，这可以被看作 Hinge 损失的“软”版本，具有无穷的边距（LeCun et al，2006）。

![](img/tex-19.jpg)

类别交叉熵损失。当需要对分数进行概率性解释时，使用类别交叉熵损失（也称为负对数似然值）。 令 $y = y_1, ..., y_n$ 是一个向量，代表标签 $1, ..., n$ 上真正的多项式分布，并让 $\hat y = \hat y_1, ..., y_n$ 为网络输出，由 softmax 激活函数进行变换，并表示类成员条件分布 $y_i = P(y = i | x)$。 类别交叉熵损失测量真实标签分布`y`与预测标签分布 $\hat y$ 之间的不相似度，并定义为交叉熵：

![](img/tex-20.jpg)

对于每个训练样例具有单个正确类别的硬分类问题，`y`是代表真实类别的单热向量。 在这种情况下，交叉熵可以简化为：

![](img/tex-21.jpg)

其中`t`（在哪儿？）是正确的类别。 这试图将正确类别`t`的概率质量设置为 1。因为已经使用 softmax 函数对分数 $\hat y$ 进行了变换并且表示条件分布，所以增加正确类别的质量意味着减小所有其他类的质量。 

交叉熵损失在神经网络文献中很常见，并且产生了多类分类器，其不仅预测最佳类别标签，而且预测可能标签上的分布。 当使用交叉熵损失时，假设网络的输出使用 softmax 变换进行变换。

排名损失。在某些情况下，我们没有根据标签而进行监督，而是正确和不正确的项目`x`和`x'`相提并论，我们的目标是对正确的项目评分，使其在不正确的项目之上。 当我们只有正面示例，并且通过破坏正面示例来产生负面示例时，会出现这样的训练情况。 这种情况下的有用的损失是基于边际的排名损失，它为一对正确和不正确示例定义：

![](img/tex-22.jpg)

`NN(x)`是网络为输入向量`x`分配的分数。 目标是对正确的输入打分（排名），高于不正确的输入，使用至少为 1 的边际值。一种常见的变体是使用排名损失的对数版本：

![](img/tex-23.jpg)

在语言任务中使用排名 Hinge 损失的例子，包括用辅助任务进行训练来获得预训练的词嵌入（参见第 5 节），其中给出了一个正确的单词序列和一个损坏的单词序列，我们的目标对正确序列评分，使其高于损坏的序列（Collobert & Weston，2008）。 同样，（Van de Cruys，2014）在选择偏好任务中使用排名损失，在该任务中，网络被训练为，将正确的动词 - 宾语对排列在错误的，自动生成的上面，并且（Weston，Bordes，Yakhnenko 和 Usunier， 2013）训练了一个模型，在信息提取环境中对正确的 (head, relation, trail) 三元组进行评分，使其高于损坏的三元组。 （Gao 等，2014）中可以找到使用排名对数损失的一个例子。 （dos Santos et al，2015）给出了排名对数损失的变体，它允许负面和正面类别拥有不同的边距。
