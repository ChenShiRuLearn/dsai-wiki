# 五、词嵌入

神经网络方法的一个主要组成部分是嵌入的使用 - 将每个特征表示为低维空间中的向量。 但向量从哪里来？ 本节将综述常用方法。

## 5.1 随机初始化

当有足够可用的监督训练数据时，可以将特征嵌入视为与其他模型参数：将嵌入向量初始化为随机值，并让网络训练过程将它们调整为“好的”向量。 随机初始化必须小心执行。 有效的 word2vec 实现（Mikolov 等，2013; Mikolov，Sutskever，Chen，Corrado 和 Dean，2013）使用的方法是，将单词向量初始化为范围 $[-\frac{1}{2d},\frac{1}{2d}]$ 其中`d`是维数。 另一种选择是使用 xavier 初始化（请参阅第 6.3 节），并使用均匀采样的值进行初始化，它来自 $[-\frac{\sqrt 6}{\sqrt d},\frac{\sqrt 6}{\sqrt d}]$。

实际上，人们通常会使用随机初始化方法，来初始化常见特征的嵌入向量，例如词性标签或单个字母，同时使用某种形式的监督式或无监督式预训练，来初始化潜在的罕见特征 ，如单个词的特征。 然后，预训练的向量可以在网络训练过程中被视为固定的，或者更通常，像随机初始化的向量一样对待，并进一步调整到手边的任务。

## 5.2 监督的任务特定的预训练

如果我们对任务 A 感兴趣，其中我们只有有限数量的标记数据（例如句法分析），但是有一个辅助任务 B（比如说词性标注），其中我们有更多的标记数据，我们可能需要预先训练我们的单词向量，使它们可以很好地预测任务 B ，然后使用训练好的向量来训练任务 A。这样，我们就可以利用我们为任务 B 准备的大量标记数据 。当训练任务 A 时，我们可以将预训练的向量视为固定的，或者将它们进一步调整以用于任务 A。另一个选项是针对两个目标联合训练，参见第 7 节来获得更多细节。


## 5.3 无监督的预训练

常见的情况是，我们没有这样的辅助任务，它拥有足够多的标记数据（或者我们想用更好的向量训练它，来启动这个辅助任务）。 在这种情况下，我们诉诸于“无监督”的方法，这些方法可以在大量未标记的文本上训练。 训练单词向量的技术基本上是监督学习的技术，但是我们并不监督我们关心的任务，而是从原始文本中创建实际上无限数量的监督训练实例，希望我们创建的任务能够匹配（ 或者足够接近）我们关心的最终任务。

无监督方法背后的关键思想是，人们希望“相似”单词具有相似的向量。尽管词汇相似性很难定义，并且通常非常依赖于任务，但目前的方法来自分布假设（Harris，1954），指出如果词语出现在相似的语境中，它们是相似的。不同的方法都创建了监督训练实例，其目标是从其上下文中预测单词，或从单词中预测上下文。

在大量未标记数据上训练词语嵌入的一个重要好处是，它为未出现在有监督训练集中的词提供了向量表示。理想情况下，这些单词的表示与训练集中出现的相关单词的表示相似，从而使模型能够更好地概括看不见的事件。因此期望的是，通过无监督算法学习的词向量之间的相似性，捕获相同方面的相似性，它对于执行网络的预期任务有用。

常见的无监督词嵌入算法包括 word2vec [13]（Mikolov 等，2013, 2013），GloVe（Pennington，Socher 和 Manning，2014）和 Collobert 和 Weston（2008, 2011）嵌入算法。 这些模型受神经网络启发，并基于随机梯度训练。 然而，它们与另一系列算法有着深厚的联系，这些算法在 NLP 和 IR 社区中演化，都基于矩阵分解（对于讨论，参见（Levy & Goldberg，2014b; Levy 等，2015））。

可以说，辅助问题的选择（基于什么样的上下文预测什么），比用于训练它们的学习方法，对结果向量影响更多。 因此，我们专注于可用的辅助问题的不同选择，并且略过训练方法的细节。 可以使用几个软件包来导出单词向量，包括 word2vec [14] 和 Gensim [15]，它们实现了基于上下文的带有词窗口的 word2vec 模型，word2vecf [16] 是 word2vec 的修改版本，允许使用任意上下文，GloVe [17] 实现 GloVe 模型。 许多预训练的单词向量也可以在网上下载。 

虽然超出了本教程的范围，但值得注意的是，除了用于初始化神经网络模型的词嵌入层之外，无监督训练算法导出的嵌入词在 NLP 中有广泛的应用。

## 5.4 训练目标

给定单词`w`及其上下文`c`，不同的算法会制定不同的辅助任务。 在所有情况下，每个单词都被表示为一个`d`维向量，被初始化为一个随机值。 训练模型来良好地执行辅助任务，产生良好的词嵌入，以将词与上下文相关联，进而将使相似词的嵌入向量彼此相似。 （Mikolov 等，2013; Mnih & Kavukcuoglu，2013）以及 GloVe（Pennington 等，2014）采用的语言建模启发式方法使用了辅助任务，其目标是预测给定的单词上下文。 这是在概率的建立中提出的，试图模拟条件概率`P(w | c)`。

其他方法将问题归约为二元分类的问题。 除了观察到的单词 - 上下文对的集合`D`之外，还从随机单词和上下文偶对中创建集合`D`。那么二元分类问题是：给定的`(w, c)`偶对是否来自`D`？ 这些方法不同于如何构造集合`D`，分类器的结构是什么，以及优化的目标是什么。 Collobert 和 Weston（2008 年，2011 年）采用基于边际的二元排名方法，训练前馈神经网络，以便对错误的`(w, c)`对进行评分。 Mikolov 等人（2013 年，2014 年）采用概率版本，训练对数双线性模型来预测概率`P ((w, c) ∈ D|w, c) `，偶对来自语料库而不是随机样本。

## 5.5 上下文的选择

在大多数情况下，单词的上下文视为出现在其周围，或者在它周围的短窗口中，或者在同一个句子，段落或文档中的其他单词。 在某些情况下，文本由语法分析器自动解析，并且上下文源自自动分析树构造的语法邻域。 有时，单词和上下文的定义也会改变，也包括单词的一部分，例如前缀或后缀。

神经词嵌入源于语言建模的世界，其中网络被训练来基于先前词的序列来预测下一个词（Bengioet 等，2003）。 其中，文本用于创建辅助任务，目标是基于`k`个先前单词的上下文来预测单词。 虽然对语言建模辅助预测问题的训练，确实产生了有用的嵌入，但是这种方法受到语言建模任务的约束的不必要的限制，其中仅仅允许查看前面的单词。 如果我们不关心语言建模而只关心所得到的嵌入，我们可以做得更好，通过忽略这个约束，并将上下文选取为焦点词周围的对称窗口。

### 5.5.1 窗口方法

最常见的方法是滑动窗口方法，其中通过查看`2k + 1`个单词的序列来创建辅助任务。 中间词被称为焦点词，每侧的`k`个词是上下文。 然后，创建单个任务，目标是基于所有上下文单词（使用 CBOW（Mikolov 等，2013）或向量连接（Collobert 和 Weston，2008））来预测焦点词。或者创建`2k` 个不同的任务，每个任务将焦点单词与不同的上下文单词配对。由（Mikolov 等，2013）推广的`2k`任务方法被称为skip-gram 模型。 基于 Skip-gram 的方法被证明是健壮且有效的（Mikolovet 等，2013; Pennington 等，2014），并且经常产生最先进的结果。

窗口大小的影响。滑动窗口的大小对所的向量相似性具有很强的影响。 较大的窗口倾向于产生更多的主题相似性（即`dog`，`bark`和`leash`将被组合在一起，以及`walked`，`run`和`walking`），而较小的窗口往往会产生 更多的功能和句法相似性（即`Poodle`，`Pitbull`，`Rottweiler`，或`walking`，`running`，`approaching`）。

位置窗口。当使用 CBOW 或 skip-gram 上下文表示时，窗口中的所有不同上下文单词都被平等对待。在焦点词附近的上下文单词和远离单词的上下文单词之间没有区别，同样，在焦点词之前出现的上下文单词与出现在它之后的上下文单词之间没有区别。通过使用位置上下文可以很容易地考虑这样的信息：为每个上下文单词指示它与焦点单词的相对位置（即，上下文单词不是`the`，变成`the: +2`，表示单词出现在焦点词右侧两个位置上）。将位置上下文与较小的窗口一起使用，倾向于产生更多语法的相似性，具有一种强烈趋势，将词性相同的词，以及语义方面功能上相似的词组合在一起。当用于初始化网络来进行词性标注和句法依赖解析时，（Ling，Dyer，Black，Trancoso，2015a）表明，位置向量比基于窗口的向量更有效。

变体。窗口方法的许多变体都是可能的。 人们可以在学习之前对单词进行词形还原，应用文本规范化，过滤太短或太长的句子，或者去除大写（例如，参见（dos Santos 和 Gatti，2014）中描述的预处理步骤。）可以对语料库的一部分进行子样本分析 ，使用某些概率跳过从窗口创建任务，它们具有过于常见或罕见的焦点词。窗口大小可能是动态的，每个回合使用不同的窗口大小。窗口中不同位置的权重可能不同，更加关注尝试 正确预测邻近的单词上下文对，而不是较远的。这些选择中的每一个都会影响所得向量。这些超参数（和其他）在（Levy 等，2015）中讨论。

### 5.5.2 句子，段落或文档

使用 skip-gram（或 CBOW）方法，可以将单词的上下文视为同一句子，段落或文档中出现的所有其他单词。 这等效于使用非常大的窗口大小，并且预计产生捕获主题相似性的词向量（来自相同主题的词，例如人们期望在同一文档中出现的词，可能接收类似的向量）。

### 5.5.3 句法窗口

一些作品用句法上下文替换句子中的线性上下文（Levy 和 Goldberg，2014a；Bansal，Gimpel，和 Livescu，2014）。 使用依赖解析器自动解析文本，并且将单词的上下文视为在解析树中接近的单词，以及它们连接的语法关系。 这些方法产生高度功能相似性，将可以在句子中起到相同作用的词组合在一起（例如颜色，学校名称，动作动词）。

分组也是语法分组，将影响相同的词汇组合在一起（Levy 和 Goldberg，2014a）。

### 5.5.4 多语言

另一种选择是使用基于翻译的多语言语境（Hermann 和 Blunsom，2014; Faruqui 和 Dyer，2014）。例如，给定大量的句子对齐的平行文本，可以运行双语对齐模型，例如 IBM 模型 1 或模型 2（即使用 GIZA ++ 软件），然后使用生成的 alignment 来导出单词上下文。这里，单词实例的上下文是与其对齐的外语单词。这种对齐倾向于导致同义词接收相似的向量。有些作者在句子对齐级别上工作，而不依赖于单词对齐。一种吸引人的方法是将单语窗口方法与多语言方法相结合，创建两种辅助任务。这可能产生与基于窗口的方法类似的向量，但是减少了基于窗口的方法的一些不希望的影响，其中反义词（例如，热和冷，高和低）倾向于接收相似的向量（Faruqui 和 Dyer，2014） 。

## 5.5 基于字符和子词表示

一个有趣的工作，尝试从组成单词的字符中导出一个它的向量表示。这些方法可能对于本质上语法上的任务特别有用，因为单词中的字符模式与它们的句法功能密切相关。这些方法还具有一些好处，产生非常小的模型（字母表中的每个字符只有一个向量，以及需要存储少量的小型矩阵），并且能够为可能遇到的每个字提供嵌入向量。 dos Santos 和 Gatti（2014）以及 dos Santos 和 Zadrozny（2014）使用卷积网络（见第 9 节）来建模字符嵌入。 Ling 等（2015b）使用两个 RNN（LSTM）编码器（第 10 节）的最终状态的串联来模拟词嵌入，一个从左到右读取字符，另一个从右到左读取字符。两者都为语音标记产生非常强大的结果。 Ballesteros 等人（2015）的工作表明，（Ling 等人，2015b）的双 LSTM 编码也有利于表示，形态丰富的语言的依赖性分析中的单词。

从字符表示中获取单词的表示是由未知单词问题引起的 - 当你遇到一个没有嵌入向量的单词时，你会怎么做？处理字符级别很大程度上缓解了这个问题，因为可能的字符比可能的单词要少得多。然而，处理字符级别是非常具有挑战性的，因为语言中的形式（字符）和函数（语法，语义）之间的关系非常松散。将自己限制在字符级别上，可能是不必要的硬约束。一些研究人员提出了一个中间立场，单词表示为单词本身的向量，以及它包含的子词单元的向量的组合。然后，子词嵌入有助于共享具有相似形式的不同单词之间的信息，并且允许单词未被观察时允许回退到子词级别。同时，当观察了足够的单词时，模型不会被迫仅仅依赖于形式。 Botha 和 Blunsom（2014）建议，将单词的嵌入向量建模为单词特定向量的总和，如果这样的向量可用，则使用不同形态的组件构成的向量（组件使用 Morfessor（Creutz 和 Lagus，2007）导出，一种无监督的形态分割方法）。Gao 等人（Gao et al。，2014）建议不仅使用单词形式本身作为核心功能，而且还使用单词中每个字母三元组的独特特征（因此是唯一的嵌入向量）。
