# 七、层叠和多任务学习

使用计算图抽象，将在线训练方法与自动梯度计算相结合，可以轻松实现模型级联，参数共享和多任务学习。

模型级联是一种强大的技术，通过将较小的组件网络组成大型网络来构建大型网络。例如，我们可以有前馈网络，用于基于其相邻单词和/或组成单词的字符来预测单词的词性。在流水线方法中，我们将使用该网络来预测语音部分，然后将预测作为输入特征，提供给进行语法分块或解析的神经网络。相反，我们可以将此网络的隐层视为一种编码，捕获用于预测词性的相关信息。在层叠方法中，我们采用该网络的隐层（而不是语音预测本身），并将它们连接起来作为语法网络的输入。我们现在有一个更大的网络，它将单词和字符作为输入序列，并输出非语法结构。计算图抽象允许我们轻松地将误差梯度从语法任务损失反向传播到字符。

为了解决深度网络的梯度消失问题，以及更好地利用可用的训练素材，可以通过在相关任务上单独训练它们，来自举网络单个部分的参数，然后将它们插入更大的网络中来进一步调整。 例如，将其隐藏层插入到可用较少训练数据的句法解析网络之前，可以训练词性预测网络，准确地预测相对大的带标注的语料库上的词性。 如果训练数据为两个任务提供直接的监督，我们可以在训练期间，通过创建具有两个输出的网络来使用它，每个任务一个，为每个输出计算单独的损失，然后将损失汇总到单个节点中。 我们从它反向传播误差梯度。

当使用卷积，递归和循环神经网络时，模型级联是非常常见的，其中，例如，使用循环网络将句子编码为固定大小的向量，然后将其用作另一网络的输入。 循环网络的监督信号主要来自上层网络，它将当前网络的输出当做输入。

当我们有相关的，不必相互馈送预测任务时，就会使用多任务学习，但我们确实认为对一种预测有用的信息对某些其他任务也很有用。例如，分块，命名实体识别（NER）和语言建模是协同任务的示例。用于预测块的边界，命名实体边界和这些内容中的下一个词的信息都依赖于一些共享的底层语法 - 语义表示。我们可以创建一个具有多个输出的单个网络，而不是为每个任务训练单独的网络。一种常见的方法是建立一个多层前馈网络，然后将最终的隐藏层（或所有隐藏层的串联）传递给不同的输出层。这样，网络的大多数参数都在不同的任务之间共享。从一个任务中学习的有用信息可以帮助消除其他任务的歧义。同样，计算图抽象使得构建这样的网络并计算它们的梯度非常容易，通过为每个可用的监督信号计算单独的损失，然后将损失相加成用于计算梯度的单个损失。如果我们有几个语料库，每个语料库都有不同类型的监督信号（例如我们有 NER 语料库和另用于分块的语料库），训练程序将支持所有可用的训练示例，对每个迭代中的不同损失计算和更新梯度。在语言处理的背景下的多任务学习在（Collobert 等，2011）介绍和讨论。
